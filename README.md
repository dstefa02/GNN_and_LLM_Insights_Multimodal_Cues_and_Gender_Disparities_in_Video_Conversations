# GNN and LLM Insights Multimodal Cues and Gender Disparities in Video Conversations

# Overview
As video content on online platforms continues to grow, understanding the nuanced aspects of interpersonal communication becomes essential—especially when it comes to uncovering gender bias in multimodal interactions. Our study investigates how visual, vocal, and verbal cues interact during video conversations and whether these cues are perceived differently based on gender.

# Key Contributions
- Semi-Automatic Feature Extraction: We developed a method to automatically extract features and domain knowledge from unstructured, user-generated video content.
- Dataset: Analysis is based on 1,091 multi-participant video conversations from Shark Tank.
- Deep Learning Approaches: Advanced deep learning models were applied to extract multimodal cues such as emotions from video, audio, and textual data.
- Graph Neural Networks (GNN): We modeled multi-participant interactions using GNNs to capture the complex relationships in conversations.
- Large Language Models (LLM): GPT-4 was utilized to simulate decision-making scenarios, offering insights into potential gender biases in text-based assessments.

# Abstract
Understanding the complex dynamics of video conversations is critical in today's digital landscape. In this work, we tackle the challenge of extracting and interpreting multimodal cues—visual, vocal, and verbal—from video content to uncover underlying gender biases. Through a semi-automatic feature extraction process applied to 1,091 video conversations from Shark Tank, we analyze how the cues of conversational participants, such as entrepreneurs and investors, are differentially affected by gender. Our methodology combines advanced deep learning techniques for cue extraction, Graph Neural Networks for modeling interactions, and GPT-4 for simulating decision-making scenarios. The results provide a comprehensive understanding of gender disparities in video conversations and highlight the role of multimodal cues in influencing these dynamics.

# Repository Structure
- /data: Contains processed datasets and instructions for data preparation.
- /code: Code for multimodal cue extraction and GNN modeling.

# Video Processing Methodology
![Fig 1 Video Processing_presentation](https://github.com/user-attachments/assets/c7ab4b91-80c8-4c2e-817a-2e66a86f4e28)

# GNN Methodology
![Fig 2 GNN_methodology](https://github.com/user-attachments/assets/f2bfcac8-4875-428b-b887-3c8424b05690)

# Citation
> @inproceedings{stefanidis2025gnn,
>   title={GNN and LLM Insights: Multimodal Cues and Gender Disparities in Video Conversations},
>   author={Dimosthenis Stefanidis, George Pallis, Marios D. Dikaiakos, and Nicos Nicolaou},
>   booktitle={Proceedings of the International Conference on Web and Social Media (ICWSM)},
>   year={2025}
> }
